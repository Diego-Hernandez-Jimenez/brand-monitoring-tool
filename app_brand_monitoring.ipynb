{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hKag-V5GtOaW",
        "OJkrnRqjtFpg",
        "-rTcfcXJtBeN"
      ],
      "authorship_tag": "ABX9TyMpB1NihuCh8lfLO9UhTI8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diego-Hernandez-Jimenez/brand-monitoring-tool/blob/main/app_brand_monitoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/Colab_Notebooks/Brand monitoring\""
      ],
      "metadata": {
        "id": "hYjJ74HzVhMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6d203d-aa6c-4160-d29f-7c34f26cebda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab_Notebooks/Brand monitoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python version\n",
        "!python --version"
      ],
      "metadata": {
        "id": "7CtfiNep2GfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install gradio --quiet\n",
        "!uv pip install bertopic --quiet\n",
        "!uv pip install groq --quiet"
      ],
      "metadata": {
        "id": "MCobGU8v2Tyb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv --version"
      ],
      "metadata": {
        "id": "EItMmMnqX4tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "altair==5.5.0\n",
        "bertopic==0.17.0\n",
        "gradio==5.31.0\n",
        "gradio-client==1.10.1\n",
        "groq==0.25.0\n",
        "hdbscan==0.8.40\n",
        "nltk==3.9.1\n",
        "numpy==2.0.2\n",
        "pandas==2.2.2\n",
        "requests==2.32.3\n",
        "safetensors==0.5.3\n",
        "scikit-learn==1.6.1\n",
        "sentence-transformers==4.1.0\n",
        "textblob==0.19.0\n",
        "umap-learn==0.5.7"
      ],
      "metadata": {
        "id": "15AFOqLQye8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import environ\n",
        "\n",
        "LANG = 'en'\n",
        "\n",
        "NEWS_API_KEY = userdata.get('news_api_key')\n",
        "DIFFBOT_API_KEY = userdata.get('diffbot_key')\n",
        "environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "EXCLUDED_SOURCES = 'stacksocial.com,bringatrailer.com,frequentmiler.com,slickdeals.net,dealcatcher.com,pypi.org'\n",
        "MAX_DAYS_AGO = 30\n",
        "MAX_ARTICLES = 100\n",
        "\n",
        "RPS_LIMIT = 5 # Requests Per Second (RPS) limit with diffbot plan\n",
        "\n",
        "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "GROQ_MODEL = 'gemma2-9b-it'\n",
        "\n",
        "NEWSAPI_ERROR_MSG = 'Something went wrong with the article search. Please try again in a moment.'\n",
        "DIFFBOT_DISCLAIMER_MSG = \"Your request is being processed. The analysis may take a few minutes, up to 30 minutes, based on your selected settings. We appreciate your patience!\"\n",
        "TOPIC_ANALYSIS_ERROR_MSG = 'Unable to complete topic analysis. Please consider increasing the sample size.'\n",
        "TOPIC_ANALYSIS_DISCLAIMER_MSG = 'The analysis is underway. Changes will appear on the page shortly.'"
      ],
      "metadata": {
        "id": "VOCS1A1_qHpj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hd1GiGVv-DBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data extraction"
      ],
      "metadata": {
        "id": "EVp-mkXptQi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data extraction I: News API"
      ],
      "metadata": {
        "id": "hKag-V5GtOaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile src/newsapiextraction.py\n",
        "\n",
        "from os import environ\n",
        "from json import load as json_load\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import pandas as pd\n",
        "# from constants import LANG, EXCLUDED_SOURCES, NEWSAPI_ERROR_MSG\n",
        "from gradio import Error as gradioError, Accordion as gradioAccordion\n",
        "\n",
        "# NEWS_API_KEY = environ.get('NEWS_API_KEY')\n",
        "\n",
        "def news_api_extraction(search_query: str, n_days_ago: int, search_filter: str | list[str]) -> tuple[list[dict], int]:\n",
        "  \"\"\"Searches for articles from the NewsAPI based on a query and date range.\n",
        "\n",
        "  Args:\n",
        "      search_query (str): The keywords or phrases to search for in the article title and body.\n",
        "      n_days_ago (int): The number of days ago to search for articles, up to a maximum of 30 days.\n",
        "      search_filter (str | list[str]): Where to search in the article.\n",
        "          Can be 'title', 'description' or list of these options (e.g., 'title,description' or ['title', 'description']).\n",
        "\n",
        "  Returns:\n",
        "      tuple[list[dict], int]: A tuple containing:\n",
        "          - A list of dictionaries, where each dictionary represents an article\n",
        "            with keys like 'title', 'url', 'markdown_title', 'publish_date', and 'source'.\n",
        "          - An integer representing the total number of articles retrieved.\n",
        "\n",
        "  Raises:\n",
        "      gradioError: If the NewsAPI request returns an error status code.\n",
        "  \"\"\"\n",
        "\n",
        "  # Keywords or phrases to search for in the article title and body.\n",
        "  today = datetime.today().strftime('%Y-%m-%d')\n",
        "  from_date = (datetime.today() - timedelta(days=n_days_ago)).strftime('%Y-%m-%d')\n",
        "\n",
        "  if isinstance(search_filter, str):\n",
        "    search_in = search_filter\n",
        "  elif isinstance(search_filter, list):\n",
        "    search_in = ','.join(search_filter)\n",
        "  else:\n",
        "    search_in = 'title,description'\n",
        "  newsapi_url = (\n",
        "      'https://newsapi.org/v2/everything?'\n",
        "      f'q={search_query}'\n",
        "      f'&language={LANG}'\n",
        "      f'&searchIn={search_in}'\n",
        "      f'&from={from_date}'\n",
        "      f'&to={today}'\n",
        "      f'&excludeDomains={EXCLUDED_SOURCES}'\n",
        "      '&sortBy=relevancy'\n",
        "      f'&apiKey={NEWS_API_KEY}'\n",
        "      )\n",
        "\n",
        "  newsapi_response = requests.get(newsapi_url)\n",
        "\n",
        "  if newsapi_response.status_code == 200:\n",
        "    newsapi_dict = newsapi_response.json()\n",
        "    contents = []\n",
        "    articles_retrieved = newsapi_dict['totalResults']\n",
        "    for article in newsapi_dict['articles']:\n",
        "      contents.append({\n",
        "          'title': article['title'],\n",
        "          'url': article['url'],\n",
        "          'markdown_title': f'[{article[\"title\"]}]({article[\"url\"]})',\n",
        "          'publish_date': datetime.strptime(article['publishedAt'], '%Y-%m-%dT%H:%M:%SZ').strftime('%d %b %Y'),\n",
        "          'source': article['source']['name']\n",
        "      })\n",
        "\n",
        "    return contents, articles_retrieved\n",
        "  else:\n",
        "    print('Error:', newsapi_response.status_code)\n",
        "    raise gradioError(NEWSAPI_ERROR_MSG, duration=10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_preliminary_search_results(search_query: str, n_days_ago: int, search_filter: str | list[str], state: dict) -> tuple[pd.DataFrame, int, int, gradioAccordion, dict]:\n",
        "  \"\"\"Generates preliminary news search results for display in a Gradio application.\n",
        "\n",
        "  This function fetches news articles based on a search query, a date range, and\n",
        "  search filters. It handles a special case for an example query to load\n",
        "  pre-saved results. The results are then processed into a pandas DataFrame\n",
        "  and returned along with other relevant information for the Gradio UI.\n",
        "\n",
        "  Args:\n",
        "      search_query (str): The keyword or phrase to search for in news articles.\n",
        "      n_days_ago (int): The number of days back from today to search for articles.\n",
        "      search_filter (str | list[str]): Specifies where to search within the articles\n",
        "          (e.g., 'title', 'description' or a combination).\n",
        "      state (dict): A dictionary representing the current state of the Gradio application,\n",
        "          used to store and retrieve data between function calls.\n",
        "\n",
        "  Returns:\n",
        "      tuple[pd.DataFrame, int, int, gradioAccordion, dict]: A tuple containing:\n",
        "          - df_display (pd.DataFrame): A DataFrame with 'Title', 'Date', and 'Source'\n",
        "            columns, formatted for display in the Gradio app.\n",
        "          - n_articles_newsapi (int): The total number of articles retrieved from NewsAPI.\n",
        "          - n_sources_newsapi (int): The number of unique news sources found.\n",
        "          - gradioAccordion: A Gradio Accordion component set to be visible and open.\n",
        "          - state (dict): The updated state dictionary, including the search query\n",
        "            and the raw NewsAPI results.\n",
        "  \"\"\"\n",
        "\n",
        "  if search_query == 'Tesla (example)':\n",
        "      with open('examples/newsapi_example_tesla30.json', 'r') as f:\n",
        "        newsapi_results = json_load(f)\n",
        "      n_articles_newsapi = 97\n",
        "  else:\n",
        "    newsapi_results, n_articles_newsapi = news_api_extraction(search_query, n_days_ago, search_filter)\n",
        "\n",
        "  df_newsapi = pd.DataFrame(newsapi_results).sort_values(\n",
        "      by='publish_date', ascending=True, key=lambda x: pd.to_datetime(x, format='%d %b %Y')\n",
        "  )\n",
        "\n",
        "  n_sources_newsapi = df_newsapi['source'].nunique()\n",
        "\n",
        "  state['search_query'] = search_query\n",
        "  state['newsapi_results'] = newsapi_results\n",
        "\n",
        "  df_display = df_newsapi[['markdown_title', 'publish_date', 'source']] \\\n",
        "  .rename(columns={'markdown_title': 'Title', 'publish_date': 'Date', 'source': 'Source'})\n",
        "\n",
        "  return [\n",
        "      df_display,\n",
        "      n_articles_newsapi,\n",
        "      n_sources_newsapi,\n",
        "      gradioAccordion(visible=True, open=True),\n",
        "      state\n",
        "  ]"
      ],
      "metadata": {
        "id": "vhzYc5atg3i7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data extraction II: Diffbot"
      ],
      "metadata": {
        "id": "OJkrnRqjtFpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile src/diffbotextraction.py\n",
        "\n",
        "from os import environ\n",
        "import requests\n",
        "from json import load as json_load, dump as json_dump\n",
        "from time import sleep\n",
        "# from constants import DIFFBOT_DISCLAIMER_MSG, RPS_LIMIT, LANG\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gradio import Info as gradioInfo, Column as gradioColumn, Dropdown as gradioDropdown\n",
        "\n",
        "# DIFFBOT_API_KEY = environ.get('DIFFBOT_API_KEY')\n",
        "\n",
        "def diffbot_extraction(newsapi_results: list[dict], max_articles: int) -> tuple[list[str], list[dict], list[str]]:\n",
        "  \"\"\"Extracts content, summaries, and sentiment from articles using the Diffbot API.\n",
        "\n",
        "  This function iterates through a list of news articles obtained from NewsAPI,\n",
        "  sends requests to the Diffbot Article API for each, and extracts the full\n",
        "  content, a summary, and sentiment analysis. It includes rate limiting\n",
        "  considerations and error handling for API requests and parsing.\n",
        "\n",
        "  Args:\n",
        "      newsapi_results (list[dict]): A list of dictionaries, where each dictionary\n",
        "          represents an article from the NewsAPI, typically containing 'title',\n",
        "          'url', 'markdown_title', 'source', and 'publish_date'.\n",
        "      max_articles (int): The maximum number of articles to extract content from.\n",
        "\n",
        "  Returns:\n",
        "      tuple[list[str], list[dict], list[str]]: A tuple containing:\n",
        "          - documents (list[str]): A list of strings, where each string is the\n",
        "            full extracted text content of an article.\n",
        "          - list_metadata (list[dict]): A list of dictionaries, where each\n",
        "            dictionary contains metadata for an extracted article, including\n",
        "            'title', 'url', 'markdown_title', 'publish_date', 'source',\n",
        "            'sentiment', and 'summary'.\n",
        "          - categories (list[str]): A flat list of all categories identified\n",
        "            across all extracted articles.\n",
        "  \"\"\"\n",
        "\n",
        "  gradioInfo(DIFFBOT_DISCLAIMER_MSG, duration=10)\n",
        "  documents = []\n",
        "  list_metadata = []\n",
        "  categories = []\n",
        "  n_extracted = 0\n",
        "  for i, article in enumerate(newsapi_results):\n",
        "    title = article['title']\n",
        "    url = article['url']\n",
        "    markdown_title = article['markdown_title']\n",
        "    source = article['source']\n",
        "    publish_date = article['publish_date']\n",
        "    diffbot_api_url = (\n",
        "        f'https://api.diffbot.com/v3/article?url={url}'\n",
        "        f'&token={DIFFBOT_API_KEY}'\n",
        "        f'&naturalLanguage=categories,sentiment,summary'\n",
        "        f'&summaryNumSentences=4'\n",
        "        f'&discussion=false'\n",
        "    )\n",
        "    # when close to RPS limit, sleep for some time to avoid hitting it\n",
        "    if i % (RPS_LIMIT - 1) == 0:\n",
        "      sleep(0.01)\n",
        "\n",
        "    print(url)\n",
        "    diffbot_response = requests.get(diffbot_api_url, headers={'accept': 'application/json'})\n",
        "    request_status = diffbot_response.status_code\n",
        "    if request_status == 200:\n",
        "      diffbot_results = diffbot_response.json()\n",
        "      parsing_status = diffbot_results.get('errorCode', 200)\n",
        "      if parsing_status == 200:\n",
        "        try:\n",
        "          diffbot_objects = diffbot_results['objects'][0]\n",
        "          if diffbot_objects['humanLanguage'] == LANG:\n",
        "            content = diffbot_objects['text']\n",
        "            metadata = {\n",
        "                'title': title,\n",
        "                'url': url,\n",
        "                'markdown_title': markdown_title,\n",
        "                'publish_date': publish_date,\n",
        "                'source': source,\n",
        "                'sentiment':diffbot_objects['sentiment'],\n",
        "                'summary': diffbot_objects['naturalLanguage']['summary']\n",
        "              }\n",
        "\n",
        "            if diffbot_objects.get('categories', None) is not None:\n",
        "              doc_categories = [category['name'] for category in diffbot_objects['categories']]\n",
        "              categories.extend(doc_categories)\n",
        "            else:\n",
        "              pass\n",
        "\n",
        "            documents.append(content)\n",
        "            list_metadata.append(metadata)\n",
        "            n_extracted += 1\n",
        "            print('Content successfully extracted:')\n",
        "            print(title)\n",
        "            print('\\n')\n",
        "            if n_extracted >= max_articles:\n",
        "              return documents, list_metadata, categories\n",
        "\n",
        "        except:\n",
        "          print('Article skipped (unknown error during extraction):')\n",
        "          print(title)\n",
        "          print('\\n')\n",
        "          continue\n",
        "      else:\n",
        "        print('Article skipped (parsing error):')\n",
        "        print(title)\n",
        "        print('\\n')\n",
        "        continue\n",
        "    else:\n",
        "      print('Article skipped (request error):')\n",
        "      print(title)\n",
        "      print('\\n')\n",
        "      continue\n",
        "\n",
        "  return documents, list_metadata, categories\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def style_title_with_tooltip(row: pd.Series) -> str:\n",
        "  \"\"\"Styles a DataFrame row, specifically the 'title' column, with a background color\n",
        "  based on sentiment and adds a tooltip displaying the sentiment score.\n",
        "\n",
        "  Args:\n",
        "      row (pd.Series): A pandas Series representing a row from a DataFrame.\n",
        "                        It is expected to contain 'sentiment_class' (str),\n",
        "                        'sentiment' (float or int), and 'title' (str) columns.\n",
        "\n",
        "  Returns:\n",
        "      str: An HTML string representing the styled title with a tooltip.\n",
        "  \"\"\"\n",
        "\n",
        "  sentiment = row['sentiment_class']\n",
        "  tooltip = f'Sentiment: {row[\"sentiment\"]}'\n",
        "\n",
        "  if sentiment == 'positive':\n",
        "      bgcolor = 'lightgreen'\n",
        "  elif sentiment == 'negative':\n",
        "      bgcolor = 'lightcoral'\n",
        "  else:\n",
        "      bgcolor = 'lightgray'\n",
        "\n",
        "  return f\"\"\"\n",
        "  <div title=\"{tooltip}\" style=\"\n",
        "      background-color: {bgcolor};\n",
        "      display: table;\n",
        "      width: 100%;\n",
        "      padding: 10px;\n",
        "  \">\n",
        "      {row[\"title\"]}\n",
        "  </div>\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def generate_final_results(max_articles: int, state: dict) -> tuple[pd.DataFrame, int, int, float, pd.DataFrame, pd.DataFrame, gradioColumn, dict]:\n",
        "  \"\"\"Generates and processes final news search results for display in a Gradio application.\n",
        "\n",
        "  This function performs the following steps:\n",
        "  1. Loads example data or extracts article content and metadata using Diffbot API.\n",
        "  2. Calculates top categories from the extracted articles.\n",
        "  3. Computes descriptive statistics such as the number of articles, unique sources,\n",
        "      and average sentiment.\n",
        "  4. Updates the application's state with the processed data.\n",
        "  5. Formats a DataFrame for display in the Gradio front end, including\n",
        "      sentiment-based styling for article titles.\n",
        "\n",
        "  Args:\n",
        "      max_articles (int): The maximum number of articles for which to extract\n",
        "          detailed content and metadata.\n",
        "      state (dict): A dictionary representing the current state of the Gradio application,\n",
        "          containing 'search_query' and 'newsapi_results'.\n",
        "\n",
        "  Returns:\n",
        "      tuple[pd.DataFrame, int, int, float, pd.DataFrame, pd.DataFrame, gradioColumn, dict]: A tuple containing:\n",
        "          - df_display (pd.DataFrame): A DataFrame with 'Title' (styled with sentiment tooltip),\n",
        "            'Date', and 'Source' columns, prepared for Gradio display.\n",
        "          - n_articles (int): The total number of articles for which content was extracted.\n",
        "          - n_sources (int): The number of unique news sources from the extracted articles.\n",
        "          - avg_sentiment (float): The average sentiment score of the extracted articles.\n",
        "          - source_counts (pd.DataFrame): A DataFrame showing the top 5 news sources by article count.\n",
        "          - top_categories (pd.DataFrame): A DataFrame showing the top 5 categories by percentage.\n",
        "          - gradioColumn: A Gradio Column component set to be visible.\n",
        "          - state (dict): The updated state dictionary, including 'documents', 'list_metadata',\n",
        "            'df_metadata', 'list_categories', and 'random_seed'.\n",
        "  \"\"\"\n",
        "\n",
        "  target = state['search_query']\n",
        "  if target == 'Tesla (example)':\n",
        "    # load example data\n",
        "    with open(f'examples/diffbot_example_documents_tesla.json', 'r', encoding='utf-8') as f:\n",
        "      documents = json_load(f)\n",
        "\n",
        "    with open(f'examples/diffbot_example_metadata_tesla.json', 'r', encoding='utf-8') as f:\n",
        "      list_metadata = json_load(f)\n",
        "\n",
        "    with open('examples/diffbot_example_categories_tesla.txt', 'r') as f:\n",
        "      list_categories = f.read().splitlines()\n",
        "\n",
        "  else:\n",
        "    # extract articles and generate summaries (included in metadata)\n",
        "    documents, list_metadata, list_categories = diffbot_extraction(state['newsapi_results'], max_articles)\n",
        "\n",
        "    # save for optional download\n",
        "    with open(f'downloads/{target}_documents.json', 'w', encoding='utf-8') as f:\n",
        "      json_dump(documents, f)\n",
        "    with open(f'downloads/{target}_metadata.json', 'w', encoding='utf-8') as f:\n",
        "      json_dump(list_metadata, f)\n",
        "\n",
        "  # get top categories\n",
        "  top_categories = pd.DataFrame(list_categories, columns=['Category']) \\\n",
        "  .value_counts(ascending=False, normalize=True) \\\n",
        "  .mul(100) \\\n",
        "  .round(3) \\\n",
        "  .reset_index() \\\n",
        "  .head(5)\n",
        "\n",
        "  # get some descriptive statistics\n",
        "  df_metadata = pd.DataFrame(list_metadata).sort_values(\n",
        "      by='publish_date', ascending=True, key=lambda x: pd.to_datetime(x, format='%d %b %Y')\n",
        "  )\n",
        "  df_metadata['sentiment_class'] = df_metadata['sentiment'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
        "  n_articles = len(df_metadata)\n",
        "  n_sources = df_metadata['source'].nunique()\n",
        "  avg_sentiment = df_metadata['sentiment'].mean().round(3)\n",
        "  source_counts = df_metadata['source'] \\\n",
        "  .value_counts() \\\n",
        "  .reset_index() \\\n",
        "  .sort_values(by='count', ascending=False) \\\n",
        "  .head(5)\n",
        "\n",
        "  # update app state\n",
        "  state['documents'] = documents\n",
        "  state['list_metadata'] = list_metadata\n",
        "  state['df_metadata'] = df_metadata\n",
        "  state['list_categories'] = list_categories\n",
        "  state['random_seed'] = np.random.randint(0,100)\n",
        "\n",
        "  # adapt dataframe for front end\n",
        "  df_display = df_metadata.copy()\n",
        "  df_display['title'] = df_display.apply(style_title_with_tooltip, axis=1)\n",
        "  df_display = df_display[['title', 'publish_date', 'source']] \\\n",
        "  .rename(columns={'title': 'Title', 'publish_date': 'Date', 'source': 'Source'})\n",
        "\n",
        "  return [\n",
        "      df_display,\n",
        "      n_articles,\n",
        "      n_sources,\n",
        "      avg_sentiment,\n",
        "      source_counts,\n",
        "      top_categories,\n",
        "      gradioColumn(visible=True),\n",
        "      state\n",
        "  ]\n",
        "\n",
        "\n",
        "def download_docs(search_query: gradioDropdown) -> str:\n",
        "  \"\"\"Download diffbot documents as json\"\"\"\n",
        "  if search_query == 'Tesla (example)':\n",
        "    return 'examples/diffbot_example_documents_tesla.json'\n",
        "  else:\n",
        "    return f'downloads/{search_query}_documents.json'\n",
        "\n",
        "\n",
        "def download_metadata(search_query: gradioDropdown) -> str:\n",
        "  \"\"\"Download diffbot metadata as json\"\"\"\n",
        "  if search_query == 'Tesla (example)':\n",
        "    return 'examples/diffbot_example_metadata_tesla.json'\n",
        "  else:\n",
        "    return f'downloads/{search_query}_metadata.json'\n"
      ],
      "metadata": {
        "id": "s8VJniGvuQvl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis"
      ],
      "metadata": {
        "id": "-rTcfcXJtBeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile src/sentimentanalysis.py\n",
        "\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "def generate_sentiment_results(state: dict) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str, pd.DataFrame, str]:\n",
        "  \"\"\"Generates and processes sentiment-related results for display in a Gradio application.\n",
        "\n",
        "  This function takes the processed article metadata from the application state\n",
        "  and computes various sentiment-based metrics, including:\n",
        "  1. Average sentiment and frequency of articles per source.\n",
        "  2. Distribution of sentiment classes (positive, neutral, negative).\n",
        "  3. Identifies the articles with the most positive and most negative sentiment,\n",
        "      along with their summaries.\n",
        "\n",
        "  Args:\n",
        "      state (dict): A dictionary representing the current state of the Gradio application,\n",
        "                    expected to contain 'df_metadata' (a pandas DataFrame with\n",
        "                    article metadata, including 'sentiment' and 'sentiment_class').\n",
        "\n",
        "  Returns:\n",
        "      tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str, pd.DataFrame, str]: A tuple containing:\n",
        "          - sentiment_by_source (pd.DataFrame): A DataFrame showing the top 10\n",
        "            sources by article frequency, with their average sentiment and frequency.\n",
        "          - pct_sentiment (pd.DataFrame): A DataFrame showing the percentage\n",
        "            distribution of sentiment classes.\n",
        "          - min_article (pd.DataFrame): A DataFrame (transposed) containing details\n",
        "            of the article with the most negative sentiment.\n",
        "          - min_summary (str): The summary of the article with the most negative sentiment.\n",
        "          - max_article (pd.DataFrame): A DataFrame (transposed) containing details\n",
        "            of the article with the most positive sentiment.\n",
        "          - max_summary (str): The summary of the article with the most positive sentiment.\n",
        "  \"\"\"\n",
        "\n",
        "  df_metadata = state['df_metadata']\n",
        "\n",
        "  # source by sentiment\n",
        "  sentiment_by_source = df_metadata \\\n",
        "  .groupby('source', as_index=False)['sentiment'] \\\n",
        "  .agg(['mean', 'count']) \\\n",
        "  .round(3) \\\n",
        "  .sort_values(by='count', ascending=False) \\\n",
        "  .head(10) \\\n",
        "  .rename(columns={\n",
        "      'source': 'Source',\n",
        "      'mean': 'Average sentiment',\n",
        "      'count':' Frequency'\n",
        "  })\n",
        "\n",
        "  # sentiment distribution\n",
        "  pct_sentiment = df_metadata \\\n",
        "  .value_counts('sentiment_class', normalize=True) \\\n",
        "  .mul(100) \\\n",
        "  .reindex(['negative', 'neutral', 'positive']) \\\n",
        "  .reset_index() \\\n",
        "  .rename(columns={\n",
        "      'sentiment_class': 'Sentiment',\n",
        "      'proportion':'% of ocurrences'\n",
        "  })\n",
        "\n",
        "\n",
        "  # most positive article and most negative article\n",
        "  max_sentiment = df_metadata['sentiment'].max()\n",
        "  min_sentiment = df_metadata['sentiment'].min()\n",
        "\n",
        "  max_min_articles = df_metadata.loc[\n",
        "      (df_metadata['sentiment'] == max_sentiment) | (df_metadata['sentiment'] == min_sentiment),\n",
        "      ['title', 'publish_date', 'source', 'sentiment', 'summary']\n",
        "  ] \\\n",
        "  .sort_values(by='sentiment') \\\n",
        "  .rename(columns={'title': 'Title', 'publish_date': 'Date', 'source': 'Source', 'sentiment': 'Sentiment'})\n",
        "  max_min_summaries = max_min_articles['summary'].values\n",
        "\n",
        "  min_article = max_min_articles.drop(columns='summary').head(1).T.reset_index()\n",
        "  min_article.columns = ['Variable', 'Worst article']\n",
        "  min_summary = max_min_summaries[0]\n",
        "\n",
        "  max_article = max_min_articles.drop(columns='summary').tail(1).T.reset_index()\n",
        "  max_article.columns = ['Variable', 'Best article']\n",
        "  max_summary = max_min_summaries[1]\n",
        "\n",
        "\n",
        "\n",
        "  return sentiment_by_source, pct_sentiment, min_article, min_summary, max_article, max_summary\n",
        "\n",
        "\n",
        "def plot_sentiment_over_time(state: dict) -> alt.Chart:\n",
        "  \"\"\"Generates an Altair chart visualizing sentiment over time, including sentiment class,\n",
        "  average sentiment, and standard deviation.\n",
        "\n",
        "  This function processes the sentiment data from the application state to create\n",
        "  a timeline visualization. It calculates the average sentiment and its standard\n",
        "  deviation per publish date, categorizes sentiment as positive, neutral, or negative,\n",
        "  and then generates an interactive Altair rule chart with error bars.\n",
        "\n",
        "  Args:\n",
        "      state (dict): A dictionary representing the current state of the Gradio application,\n",
        "                    expected to contain 'df_metadata' (a pandas DataFrame with\n",
        "                    article metadata, including 'publish_date', 'sentiment', and\n",
        "                    'sentiment_class').\n",
        "\n",
        "  Returns:\n",
        "      alt.Chart: An Altair chart object displaying sentiment over time, with\n",
        "                  sentiment class colored rules, average sentiment, and error bars\n",
        "                  representing the standard deviation of sentiment. The chart is\n",
        "                  interactive for zooming and panning.\n",
        "  \"\"\"\n",
        "\n",
        "  df_sentiment_by_date = state['df_metadata'] \\\n",
        "  .groupby('publish_date', as_index=False) \\\n",
        "  .agg(\n",
        "      avg_sentiment=('sentiment', 'mean'),\n",
        "      std_sentiment=('sentiment', 'std'),\n",
        "      article_count=('sentiment', 'count')\n",
        "  ) \\\n",
        "  .assign(\n",
        "      sentiment_class=lambda x: x['avg_sentiment'].apply(\n",
        "          lambda y: 'positive' if y > 0 else 'negative' if y < 0 else 'neutral'\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Set neutral sentiment to a small positive value for visibility\n",
        "  df_sentiment_by_date['avg_sentiment_smooth'] = df_sentiment_by_date['avg_sentiment'].copy()\n",
        "  df_sentiment_by_date.loc[df_sentiment_by_date['sentiment_class'] == 'neutral', 'avg_sentiment_smooth'] = 0.02\n",
        "\n",
        "\n",
        "  df_sentiment_by_date['publish_date'] = pd.to_datetime(df_sentiment_by_date['publish_date'], format='%d %b %Y')\n",
        "\n",
        "  # Convert datetime to numerical representation (timestamp) for Altair scale\n",
        "  min_date_timestamp = df_sentiment_by_date['publish_date'].min().timestamp() * 1000\n",
        "  max_date_timestamp = df_sentiment_by_date['publish_date'].max().timestamp() * 1000\n",
        "\n",
        "  # Add a margin before the first date on the x-axis\n",
        "  date_range = max_date_timestamp - min_date_timestamp\n",
        "  start_margin = date_range * 0.05  # 5% margin\n",
        "  adjusted_min_date_timestamp = min_date_timestamp - start_margin\n",
        "  adjusted_max_date_timestamp = max_date_timestamp + start_margin\n",
        "\n",
        "  # Define color scale based on sentiment_class\n",
        "  color_scale = alt.Scale(domain=['positive', 'neutral', 'negative'],\n",
        "                          range=['green', 'gray', 'red'])\n",
        "\n",
        "  # Create the Altair plot\n",
        "  chart = alt.Chart(df_sentiment_by_date) \\\n",
        "  .mark_rule(strokeWidth=20) \\\n",
        "  .encode(\n",
        "      x=alt.X('publish_date',\n",
        "              axis=alt.Axis(format=\"%b %d %Y\", labelAngle=-45),\n",
        "              title='Publish Date', # added format and labelAngle\n",
        "              scale=alt.Scale(domainMin=adjusted_min_date_timestamp, domainMax=adjusted_max_date_timestamp)\n",
        "              ),\n",
        "      y=alt.Y('avg_sentiment_smooth', title='Sentiment'),\n",
        "      color=alt.Color('sentiment_class',\n",
        "                      scale=color_scale,\n",
        "                      legend=alt.Legend(title=\"Sentiment\",\n",
        "                                        direction='horizontal',\n",
        "                                        orient='none',\n",
        "                                        titleAnchor='middle',\n",
        "                                        legendX=375, legendY=-40)),\n",
        "      tooltip=['publish_date', 'avg_sentiment', 'sentiment_class', 'article_count', 'std_sentiment']\n",
        "  ) \\\n",
        "  .properties(\n",
        "      width=900,\n",
        "      height=350\n",
        "  ) \\\n",
        "  .interactive()\n",
        "\n",
        "  # Add error bars\n",
        "  error_bars = alt.Chart(df_sentiment_by_date).mark_errorbar(extent='stderr').encode( # changed to stderr\n",
        "      x='publish_date',\n",
        "      y=alt.Y('avg_sentiment_smooth', title=''),\n",
        "      yError='std_sentiment',\n",
        "      color=alt.Color('sentiment_class', scale=color_scale, legend=None),\n",
        "      tooltip=['std_sentiment']\n",
        "  )\n",
        "  final_chart = chart + error_bars\n",
        "\n",
        "  return final_chart"
      ],
      "metadata": {
        "id": "Pr8MYwY9HT2h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic analysis"
      ],
      "metadata": {
        "id": "uyVUYv5Wx352"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile src/topicanalysis.py\n",
        "\n",
        "from re import findall\n",
        "from textblob import TextBlob\n",
        "from textblob.tokenizers import BaseTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from plotly.graph_objects import Figure as plotlyFigure\n",
        "\n",
        "from nltk.downloader import download\n",
        "download('punkt_tab')\n",
        "download('wordnet')\n",
        "download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# from constants import EMBEDDING_MODEL, TOPIC_ANALYSIS_DISCLAIMER_MSG, TOPIC_ANALYSIS_ERROR_MSG, GROQ_MODEL\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import MaximalMarginalRelevance\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from gradio import Info as gradioInfo, Error as gradioError, Textbox as gradioTextbox\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "class CustomTokenizer(BaseTokenizer):\n",
        "\n",
        "  def tokenize(self, text: str) -> list[str]:\n",
        "      \"\"\"Apply sklearn regex pattern to extract words\"\"\"\n",
        "\n",
        "      words = findall(r\"(?u)\\b\\w\\w+\\b\", text)\n",
        "      return words\n",
        "\n",
        "def custom_tokenizer(text) -> list:\n",
        "  \"\"\"Custom tokenizer to remove plural nouns (with exceptions) and lemmatize verbs\"\"\"\n",
        "\n",
        "  plural_exceptions = {\"police\", \"data\", \"fish\", \"sheep\", \"species\", \"news\", \"media\"} # exceptions suggested by chatgpt\n",
        "  blob = TextBlob(text, tokenizer=CustomTokenizer())\n",
        "\n",
        "  return [\n",
        "      word.singularize() if tag == 'NNS' and word not in plural_exceptions\n",
        "      else word.lemmatize(pos='v') if tag.startswith('V')\n",
        "      else word\n",
        "      for word,tag in blob.tags\n",
        "  ]\n",
        "\n",
        "\n",
        "\n",
        "def initialize_bertopic_components(n_docs: int, random_seed: int) -> dict:\n",
        "  \"\"\"Initializes and returns a dictionary of BERTopic model components.\n",
        "\n",
        "  This function sets up the various models required for BERTopic, including\n",
        "  the embedding model, dimensionality reduction, clustering, vectorization,\n",
        "  c-TF-IDF, and representation models.\n",
        "\n",
        "  Args:\n",
        "      n_docs (int): The number of documents to be processed, used to determine\n",
        "                    the minimum cluster size for HDBSCAN.\n",
        "      random_seed (int): A seed for reproducibility in models that involve\n",
        "                          randomness, such as UMAP.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary containing initialized instances of the BERTopic components:\n",
        "            'embedding_model' (SentenceTransformer),\n",
        "            'dimensionality_reduction_model' (UMAP),\n",
        "            'clustering_model' (HDBSCAN),\n",
        "            'vectorizer_model' (CountVectorizer),\n",
        "            'ctfidf_model' (ClassTfidfTransformer), and\n",
        "            'representation_model' (MaximalMarginalRelevance).\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Pre-calculate embeddings\n",
        "  embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "  # Dimensionality Reduction\n",
        "  dimensionality_reduction_model = UMAP(\n",
        "      n_neighbors=5,\n",
        "      n_components=5,\n",
        "      min_dist=0.0,\n",
        "      metric='cosine',\n",
        "      random_state=random_seed\n",
        "  )\n",
        "\n",
        "  # Clustering\n",
        "  min_size = max(int(n_docs / 10), 2)\n",
        "  clustering_model = HDBSCAN(\n",
        "      min_cluster_size=min_size,\n",
        "      metric='euclidean',\n",
        "      cluster_selection_method='eom',\n",
        "      prediction_data=True\n",
        "  )\n",
        "\n",
        "  # Vectorize\n",
        "  vectorizer_model = CountVectorizer(\n",
        "      stop_words='english',\n",
        "      ngram_range=(1, 1),\n",
        "      tokenizer=custom_tokenizer,\n",
        "      max_df=.95,\n",
        "      min_df=0.05\n",
        "  )\n",
        "\n",
        "  # c-TF-IDF\n",
        "  ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
        "\n",
        "  # Representation Model\n",
        "  representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "  return {\n",
        "      'embedding_model': embedding_model,\n",
        "      'dimensionality_reduction_model': dimensionality_reduction_model,\n",
        "      'clustering_model': clustering_model,\n",
        "      'vectorizer_model': vectorizer_model,\n",
        "      'ctfidf_model': ctfidf_model,\n",
        "      'representation_model': representation_model\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "def run_topic_analysis(state: dict) -> dict:\n",
        "  \"\"\"Performs BERTopic topic analysis on the extracted article summaries.\n",
        "\n",
        "  This function either loads a pre-trained BERTopic model and embeddings for\n",
        "  an example search query or initializes and trains a new BERTopic model\n",
        "  using the components set up in `initialize_bertopic_components`.\n",
        "  It then generates human-readable topic labels and updates the application state\n",
        "  with the trained topic model and encoded documents. It also provides a Gradio info message\n",
        "  to indicate the start of the analysis.\n",
        "\n",
        "  Args:\n",
        "      state (dict): A dictionary representing the current state of the Gradio application.\n",
        "                    It should contain:\n",
        "                    - 'search_query' (str): The current search query.\n",
        "                    - 'documents' (list): A list of raw document texts (if a new model is trained).\n",
        "                    - 'list_metadata' (list[dict]): A list of dictionaries containing\n",
        "                      article metadata, where each dictionary should have a 'summary' key.\n",
        "                    - 'random_seed' (int): A seed for reproducibility.\n",
        "\n",
        "  Returns:\n",
        "      dict: The updated state dictionary, including:\n",
        "            - 'topic_model' (BERTopic): The trained BERTopic model.\n",
        "            - 'encoded_docs' (np.ndarray): The encoded document embeddings.\n",
        "\n",
        "  Raises:\n",
        "      gradioError: If the topic model fitting process fails.\n",
        "  \"\"\"\n",
        "\n",
        "  gradioInfo(TOPIC_ANALYSIS_DISCLAIMER_MSG, duration=5)\n",
        "\n",
        "  if state['search_query'] == 'Tesla (example)':\n",
        "    # load topic model\n",
        "    topic_model = BERTopic.load('examples/topic_model_example_tesla', embedding_model=EMBEDDING_MODEL)\n",
        "    encoded_docs = np.load('examples/example_encoded_summaries_tesla.npy')\n",
        "  else:\n",
        "    n_docs = len(state['documents'])\n",
        "    random_seed = state['random_seed']\n",
        "    params = initialize_bertopic_components(n_docs, random_seed)\n",
        "\n",
        "    topic_model = BERTopic(\n",
        "        umap_model=params['dimensionality_reduction_model'],\n",
        "        hdbscan_model=params['clustering_model'],\n",
        "        vectorizer_model=params['vectorizer_model'],\n",
        "        ctfidf_model=params['ctfidf_model'],\n",
        "        representation_model=params['representation_model'],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    summaries = [doc['summary'] for doc in state['list_metadata']]\n",
        "    encoded_docs = params['embedding_model'].encode(summaries, show_progress_bar=False)\n",
        "    # topics, probs = topic_model.fit_transform(summaries, encoded_docs)\n",
        "    try:\n",
        "      topic_model.fit(summaries, encoded_docs)\n",
        "    except Exception as e:\n",
        "      print('Cannot fit topic model')\n",
        "      raise gradioError(TOPIC_ANALYSIS_ERROR_MSG, duration=15)\n",
        "\n",
        "  topic_labels = topic_model.generate_topic_labels(\n",
        "      nr_words=3,\n",
        "      topic_prefix=False,\n",
        "      word_length=20,\n",
        "      separator='-'\n",
        "  )\n",
        "  topic_model.set_topic_labels(topic_labels)\n",
        "  state['topic_model'] = topic_model\n",
        "  state['encoded_docs'] = encoded_docs\n",
        "\n",
        "  return state\n",
        "\n",
        "\n",
        "\n",
        "def promtp_generator(keywords: str, representative_docs: str) -> str:\n",
        "  \"\"\"Imitation of ChatPromptTemplate from Langchain. It uses a \"skeleton\" prompt with placeholders to create custom prompts\"\"\"\n",
        "\n",
        "  return f\"\"\"You are given a topic described by the following keywords: {keywords}. Here are some representative documents related to this topic:\\n{representative_docs}\n",
        "\n",
        "  Based on the keywords and documents, provide a concise topic description and nothing else. Keep it under ten words\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_topic_titles(topic_model_df: pd.DataFrame, topic_model: BERTopic) -> list[str]:\n",
        "  \"\"\"Generates concise titles for topics using a language model (Groq API).\n",
        "\n",
        "  This function iterates through a DataFrame of topics, extracts keywords and\n",
        "  representative documents for a subset of topics, and then uses a language\n",
        "  model (via the Groq API) to generate a concise, human-readable title for\n",
        "  each of these topics.\n",
        "\n",
        "  Args:\n",
        "      topic_model_df (pd.DataFrame): A DataFrame containing topic information,\n",
        "                                      expected to have at least 'Topic' (int)\n",
        "                                      and 'CustomName' (str) columns.\n",
        "      topic_model (BERTopic): The trained BERTopic model instance, used to\n",
        "                              retrieve representative documents for each topic.\n",
        "\n",
        "  Returns:\n",
        "      list[str]: A list of generated topic titles (strings).\n",
        "  \"\"\"\n",
        "\n",
        "  groq_client = Groq()\n",
        "  unique_topics = topic_model_df['Topic'].values.tolist()\n",
        "  topic_raw_labels = topic_model_df['CustomName'].values.tolist()\n",
        "  offset = 1 if -1 in unique_topics else 0\n",
        "\n",
        "  n_topics_to_label = min(4, len(unique_topics) - offset)\n",
        "  topic_titles = []\n",
        "  for topic_id in unique_topics[offset:(n_topics_to_label + 1)]:\n",
        "    pretty_topic_keywords = topic_raw_labels[topic_id + offset].replace('-', ', ')\n",
        "    pretty_representative_docs = '\\n'.join(['-' + doc for doc in topic_model.get_representative_docs(topic=topic_id)])\n",
        "    llm_response = groq_client.chat.completions.create(\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": promtp_generator(pretty_topic_keywords, pretty_representative_docs),\n",
        "        }\n",
        "      ],\n",
        "      model=GROQ_MODEL,\n",
        "      temperature=0.3\n",
        "    )\n",
        "    # clean title\n",
        "    topic_title = llm_response.choices[0].message.content.replace('\\n', '').strip()\n",
        "    topic_titles.append(topic_title)\n",
        "\n",
        "  return topic_titles\n",
        "\n",
        "def plot_topic_word_scores(topic_scores: dict[int, list[tuple[str, float]]], max_topics_to_display: int = 4, num_terms_to_display: int = 5) -> alt.Chart:\n",
        "  \"\"\"Generates an Altair horizontal bar chart visualizing the top words and their scores for multiple topics.\n",
        "\n",
        "  Takes a dictionary of topic word scores, filters and sorts them,\n",
        "  and then creates a concatenated Altair chart. Each individual chart within the\n",
        "  concatenation represents a single topic, displaying its most important words\n",
        "  and their corresponding scores. The x-axis is scaled consistently across all\n",
        "  topic charts.\n",
        "\n",
        "  Args:\n",
        "      topic_scores (dict[int, list[tuple[str, float]]]): A dictionary where keys are\n",
        "          topic IDs (integers) and values are lists of tuples. Each tuple contains\n",
        "          a word (string) and its associated score (float) for that topic.\n",
        "      max_topics_to_display (int, optional): The maximum number of topics to display\n",
        "          in the concatenated chart. Defaults to 4.\n",
        "      num_terms_to_display (int, optional): The number of top terms (words) to display\n",
        "          for each topic. Defaults to 5.\n",
        "\n",
        "  Returns:\n",
        "      alt.Chart: An Altair horizontal concatenated bar chart object, showing the\n",
        "                  top words and their scores for the specified number of topics.\n",
        "                  The chart includes a main title and individual topic titles.\n",
        "  \"\"\"\n",
        "\n",
        "  # Altair's 'tableau10' scheme colors.\n",
        "  colors = [\n",
        "      '#4C78A8', '#F58518', '#E4572E', '#72B7B2', '#54A24B',\n",
        "      '#EECA3B', '#B279A2', '#FF9DA7', '#9D755D', '#BAB0AC'\n",
        "  ]\n",
        "  # this will be used to decide which color to use first\n",
        "  if -1 in topic_scores.keys():\n",
        "    offset = 1\n",
        "  else:\n",
        "    offset = 0\n",
        "\n",
        "  charts = []\n",
        "  # Get topic IDs, filter out topic -1, sort them to ensure consistent order,\n",
        "  # and then limit to the specified maximum number of topics to display.\n",
        "  filtered_topic_ids = [topic_id for topic_id in topic_scores.keys() if topic_id != -1]\n",
        "  sorted_topic_ids = sorted(filtered_topic_ids)[:max_topics_to_display]\n",
        "\n",
        "  # Determine the maximum score across all displayed topics for consistent x-axis scaling\n",
        "  max_score = 0\n",
        "  for topic_id in sorted_topic_ids:\n",
        "    # Consider only the top N terms for max_score calculation as well\n",
        "    for _, score in topic_scores[topic_id][:num_terms_to_display]:\n",
        "      if score > max_score:\n",
        "          max_score = score\n",
        "\n",
        "  # Add a small buffer to the max_score for the x-axis domain\n",
        "  x_axis_domain_max = max_score * 1.1 if max_score > 0 else 0.4 # Ensure at least 0.4 if no data\n",
        "\n",
        "  # Iterate through the selected topic IDs to create individual charts\n",
        "  for i, topic_id in enumerate(sorted_topic_ids):\n",
        "    # Get the words and scores for the current topic, taking only the top N terms\n",
        "    words_scores = topic_scores[topic_id][:num_terms_to_display]\n",
        "\n",
        "    # Create a pandas DataFrame for the current topic's data\n",
        "    df = pd.DataFrame(words_scores, columns=['word', 'score'])\n",
        "    df['topic'] = f'Topic {topic_id}' # Keep 'topic' column for color encoding\n",
        "\n",
        "    # Assign a color from the predefined list, cycling through if necessary\n",
        "    color_for_topic = colors[i + offset % len(colors)]\n",
        "\n",
        "    # Create a bar chart for the current topic\n",
        "    chart = alt.Chart(df) \\\n",
        "    .mark_bar(cornerRadius=3) \\\n",
        "    .encode(\n",
        "        x=alt.X('score', title=None, scale=alt.Scale(domain=(0, x_axis_domain_max)), axis=alt.Axis(labelFontSize=12)),\n",
        "        y=alt.Y('word', sort='-x', title=None, axis=alt.Axis(labels=True, ticks=False, labelFontSize=14)),\n",
        "        color=alt.value(color_for_topic),\n",
        "        tooltip=['word', 'score']\n",
        "    ) \\\n",
        "    .properties(title=alt.Title(f'Topic {topic_id}', fontSize=16),\n",
        "                width=150,\n",
        "                height=125)\n",
        "\n",
        "    charts.append(chart)\n",
        "\n",
        "  # If no charts were created (e.g., topic_scores was empty, max_topics_to_display was 0,\n",
        "  # or only topic -1 was present)\n",
        "  # if not charts:\n",
        "  #     return alt.Chart(pd.DataFrame({'message': ['No topics to display.']})).mark_text(\n",
        "  #         align='center', baseline='middle', fontSize=20, color='gray'\n",
        "  #     ).encode(\n",
        "  #         text='message'\n",
        "  #     ).properties(\n",
        "  #         title=\"Topic Word Scores\"\n",
        "  #     )\n",
        "\n",
        "  # Concatenate all individual topic charts horizontally\n",
        "  final_chart = alt \\\n",
        "  .hconcat(*charts, spacing=10) \\\n",
        "  .resolve_scale(x='shared') \\\n",
        "  .properties(title=alt.Title(\"Topic Word Scores\", anchor=\"middle\", fontSize=26, dy=-10)) \\\n",
        "  .configure_axis(grid=True, gridColor='#E0E0E0') \\\n",
        "  .configure_view(stroke=None) # Remove border around the entire plot\n",
        "\n",
        "  return final_chart\n",
        "\n",
        "\n",
        "def plot_topics_scatterplot(topic_model: BERTopic,\n",
        "                            encoded_docs: np.ndarray,\n",
        "                            summaries: list[str],\n",
        "                            topic_label_map: dict,\n",
        "                            random_seed: int) -> alt.Chart:\n",
        "\n",
        "  \"\"\"Plots documents in a 2D scatter plot, colored by their assigned topic.\n",
        "\n",
        "  This function takes document embeddings, reduces their dimensionality to 2D using UMAP,\n",
        "  and then creates an Altair scatter plot. Each point represents a document, and its\n",
        "  color indicates its topic. Topic centroids are also plotted with labels.\n",
        "\n",
        "  Args:\n",
        "      topic_model (BERTopic): The trained BERTopic model, used to get topic assignments.\n",
        "      encoded_docs (np.ndarray): A NumPy array of document embeddings.\n",
        "      summaries (list[str]): A list of document summaries, used for tooltips.\n",
        "      topic_label_map (dict): A dictionary mapping topic IDs to their human-readable labels.\n",
        "      random_seed (int): A seed for reproducibility in UMAP.\n",
        "\n",
        "  Returns:\n",
        "      alt.Chart: An Altair chart object representing the 2D scatter plot of documents,\n",
        "                  colored by topic, with topic centroid labels. The chart is interactive.\n",
        "  \"\"\"\n",
        "\n",
        "  topic_per_doc = topic_model.topics_\n",
        "  df = pd.DataFrame({'topic': topic_per_doc, 'summary': summaries})\n",
        "\n",
        "  # Reduce dimensionality of embeddings\n",
        "  umap_model = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=random_seed).fit(encoded_docs)\n",
        "  embeddings_2d = umap_model.embedding_\n",
        "\n",
        "  # Combine data\n",
        "  df['x'] = embeddings_2d[:, 0]\n",
        "  df['y'] = embeddings_2d[:, 1]\n",
        "\n",
        "  # Add topic labels for viualization (legend)\n",
        "  df['topic_label'] = df['topic'].map(topic_label_map)\n",
        "\n",
        "\n",
        "  centroids = df.groupby('topic')[['x', 'y']].mean().reset_index()\n",
        "  # Add topic labels for visualization (centroids)\n",
        "  # Same as labels in legend, but without the prefix \"Topic j:\"\n",
        "  centroids['topic_label'] = centroids['topic'].map(topic_label_map).str.split(': ').str[1]\n",
        "\n",
        "  # Create the Altair plot\n",
        "  base = alt.Chart(df) \\\n",
        "  .encode(\n",
        "      x=alt.X('x',\n",
        "              title='Dimension 1',\n",
        "              scale=alt.Scale(domain=[df['x'].min() - 0.5, df['x'].max() + 0.5]) # Adjust the domain\n",
        "              ),\n",
        "      y=alt.Y('y',\n",
        "              title='Dimension 2',\n",
        "              scale=alt.Scale(domain=[df['y'].min() - 0.5, df['y'].max() + 0.5]) # Adjust the domain\n",
        "              ),\n",
        "      color=alt.Color('topic_label', title='Topic'), # Use topic_label for the legend, with title 'Topic'\n",
        "      tooltip=['topic_label', 'summary']  # Include topic label in the tooltip\n",
        "  ) \\\n",
        "  .properties(\n",
        "      title='Document Representation in 2-D space', # Add a title\n",
        "      width=700,  # Set the width of the chart\n",
        "      height=400   # Set the height of the chart\n",
        "  ) \\\n",
        "  .interactive()\n",
        "\n",
        "  points = base.mark_circle(size=100, stroke='black', strokeWidth=2, opacity=.9) # Increase size and add border\n",
        "\n",
        "  # Add labels for the centroids, exclude label for topic -1\n",
        "  labels = alt.Chart(centroids) \\\n",
        "  .mark_text(\n",
        "      align='left',\n",
        "      baseline='middle',\n",
        "      dx=7,  # Nudge labels to the right\n",
        "      dy=-8,  # Nudge labels slightly up\n",
        "      fontWeight='bold'\n",
        "  ) \\\n",
        "  .encode(\n",
        "      x='x',\n",
        "      y='y',\n",
        "      text=alt.condition(  # Use a condition to control the text\n",
        "          alt.datum.topic != -1,  # If topic is not -1, display the label\n",
        "          'topic_label',         # ...display the topic_label\n",
        "          alt.value('')       # ...otherwise, display an empty string\n",
        "      ),\n",
        "      color=alt.value('black')\n",
        "  ) \\\n",
        "  .interactive()\n",
        "\n",
        "  chart = points + labels\n",
        "  topic_scatter_chart = chart.configure_title(\n",
        "      fontSize=18,\n",
        "      font='Arial',\n",
        "      color='black'\n",
        "  )\n",
        "\n",
        "  return topic_scatter_chart\n",
        "\n",
        "\n",
        "\n",
        "def generate_topic_results(state: dict) -> tuple[plotlyFigure, alt.Chart, gradioTextbox, gradioTextbox, gradioTextbox, gradioTextbox]:\n",
        "  \"\"\"Generates and displays topic analysis results in a Gradio application.\n",
        "\n",
        "  This function leverages a pre-trained or newly trained BERTopic model to\n",
        "  produce various visualizations and textual outputs related to identified topics.\n",
        "  It generates a barchart of topic word scores, a 2D scatter plot of documents\n",
        "  colored by topic, and concise titles for the prominent topics using a language model.\n",
        "\n",
        "  Args:\n",
        "      state (dict): A dictionary representing the current state of the Gradio application.\n",
        "                    It is expected to contain:\n",
        "                    - 'topic_model' (BERTopic): The trained BERTopic model.\n",
        "                    - 'random_seed' (int): The random seed used for reproducibility.\n",
        "                    - 'list_metadata' (list[dict]): A list of dictionaries with article metadata,\n",
        "                      each containing a 'summary' key.\n",
        "                    - 'encoded_docs' (np.ndarray): The encoded document embeddings.\n",
        "                    - 'search_query' (str): The current search query, used to load example data.\n",
        "\n",
        "  Returns:\n",
        "      tuple[go.Figure, alt.Chart, gradioTextbox, gradioTextbox, gradioTextbox, gradioTextbox]: A tuple containing:\n",
        "          - topic_barchart (go.Figure): A Plotly bar chart visualizing topic word scores.\n",
        "          - topic_scatterplot (alt.Chart): An Altair scatter plot of documents in 2D space, colored by topic.\n",
        "          - gr_textboxes (gradioTextbox): Four Gradio Textbox components, displaying generated\n",
        "            topic titles. If fewer than 4 titles are generated, the remaining textboxes will be invisible.\n",
        "  \"\"\"\n",
        "\n",
        "  topic_model = state['topic_model']\n",
        "  random_seed = state['random_seed']\n",
        "  list_metadata = state['list_metadata']\n",
        "  summaries = [doc['summary'] for doc in list_metadata]\n",
        "  encoded_docs = state['encoded_docs']\n",
        "\n",
        "  topic_model_df = topic_model.get_topic_info()\n",
        "  topic_label_map = {}\n",
        "  for i, row in topic_model_df.iterrows():\n",
        "    topic_id = row['Topic']\n",
        "    topic_label = row['CustomName']\n",
        "    if topic_id == -1:\n",
        "      topic_label_map[topic_id] = f'Topic -1: {topic_label} (noise)'\n",
        "    else:\n",
        "      topic_label_map[topic_id] = f'Topic {topic_id}: {topic_label}'\n",
        "\n",
        "  # barchart\n",
        "  topic_barchart = plot_topic_word_scores(topic_model.get_topics(), max_topics_to_display=4, num_terms_to_display=5)\n",
        "\n",
        "  # scatterplot\n",
        "  topic_scatterplot = plot_topics_scatterplot(topic_model, encoded_docs, summaries, topic_label_map, random_seed)\n",
        "\n",
        "  # topic titles\n",
        "  if state['search_query'] == 'Tesla (example)':\n",
        "    with open('examples/example_topic_titles_tesla.txt', 'r') as f:\n",
        "      topic_titles = f.read().splitlines()\n",
        "  else:\n",
        "    topic_titles = get_topic_titles(topic_model_df, topic_model)\n",
        "\n",
        "  n_generated_titles = len(topic_titles)\n",
        "  n_empty_textboxes = 4 - n_generated_titles\n",
        "  gr_textboxes = []\n",
        "  for i in range(n_generated_titles):\n",
        "    gr_textboxes.append(gradioTextbox(label=f'Topic {i}', value=topic_titles[i], visible=True))\n",
        "  for i in range(n_empty_textboxes):\n",
        "    gr_textboxes.append(gradioTextbox(visible=False))\n",
        "\n",
        "\n",
        "  return topic_barchart, topic_scatterplot, gr_textboxes[0], gr_textboxes[1], gr_textboxes[2], gr_textboxes[3]"
      ],
      "metadata": {
        "id": "kI_LpZIPICK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6d004d-1cec-466c-d7c8-99835527c542"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/topicanalysis.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio app"
      ],
      "metadata": {
        "id": "KV4f140n5Uds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app_brand_monitoring.py\n",
        "\n",
        "# from constants import MAX_ARTICLES, MAX_DAYS_AGO\n",
        "# from newsapiextraction import generate_preliminary_search_results\n",
        "# from diffbotextraction import generate_final_results, download_docs, download_metadata\n",
        "# from sentimentanalysis import generate_sentiment_results, plot_sentiment_over_time\n",
        "# from topicanalysis import run_topic_analysis, generate_topic_results\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def load_top_brands() -> list[str]:\n",
        "  \"\"\"Load top 50 brands from txt file and sort by name\"\"\"\n",
        "\n",
        "  with open('examples/top_brands.txt') as f:\n",
        "    top_brands = f.read().splitlines()[:50]\n",
        "    top_brands.append('Tesla (example)')\n",
        "    top_brands.sort()\n",
        "\n",
        "  return top_brands\n",
        "\n",
        "\n",
        "def allow_topic_analysis(n_articles_label: str) -> gr.Button:\n",
        "  \"\"\"Allow topic analysis only if there are enough articles\"\"\"\n",
        "\n",
        "  if int(n_articles_label) >= 6:\n",
        "    return gr.Button(value='Run topic analysis 🔓', interactive=True)\n",
        "  else:\n",
        "    return gr.Button(value='Not enough sample to run topic analysis 🔒', interactive=False)\n",
        "\n",
        "custom_theme = gr.Theme.load('gradio_theme/JohnSmith9982small_and_pretty.json')\n",
        "with gr.Blocks(theme=custom_theme) as demo:\n",
        "  app_state = gr.State({})\n",
        "  top_brands = load_top_brands()\n",
        "\n",
        "  gr.Markdown('# Brand Monitoring Tool')\n",
        "  with gr.Tab('Search page') as search_page_tab:\n",
        "    with gr.Row():\n",
        "      gr.Markdown('Stay updated with the latest news about the most important brands. Select a brand, search for related articles, and analyze the content to gain insights into sentiment, article sources, and more.')\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        with gr.Row():\n",
        "          search_query = gr.Dropdown(\n",
        "              label='Brand selection',\n",
        "              choices=top_brands,\n",
        "              interactive=True,\n",
        "              value='Tesla (example)',\n",
        "              scale=2\n",
        "          )\n",
        "          search_filter = gr.CheckboxGroup(\n",
        "              label='Search matches in ...',\n",
        "              choices=['title', 'description'],\n",
        "              value = ['title', 'description'],\n",
        "              interactive=True,\n",
        "              scale=1\n",
        "          )\n",
        "        n_days_ago = gr.Number(\n",
        "            label='Get articles from last n days (max = 30)',\n",
        "            minimum=1,\n",
        "            maximum=MAX_DAYS_AGO, # we limit the upper bound to\n",
        "            step=1,\n",
        "            value=MAX_DAYS_AGO,\n",
        "            interactive=True\n",
        "        )\n",
        "        max_articles = gr.Number(\n",
        "            label='Maximum number of articles to analyze (max = 100)',\n",
        "            minimum=1,\n",
        "            maximum=MAX_ARTICLES, # we limit the upper bound to\n",
        "            step=5,\n",
        "            value=MAX_ARTICLES,\n",
        "            interactive=True\n",
        "        )\n",
        "        search_button = gr.Button('Search 🔍')\n",
        "        with gr.Accordion('Preliminary search results', open=False) as newsapi_accordion:\n",
        "          with gr.Row():\n",
        "            number_of_preliminary_articles = gr.Label(label='# of retrieved articles')\n",
        "            number_of_preliminary_sources = gr.Label(label='# of different sources')\n",
        "          preliminary_results = gr.DataFrame(\n",
        "              label='Retrieved articles (This list might not be definitive)',\n",
        "              headers=['Title', 'Date', 'Source'],\n",
        "              col_count=3,\n",
        "              column_widths=['60%', '20%', '20%'],\n",
        "              show_row_numbers=False,\n",
        "              wrap=True,\n",
        "              datatype='markdown'\n",
        "          )\n",
        "          extract_button = gr.Button('Extract content from articles', icon=None, visible=False)\n",
        "      with gr.Column(visible=False) as extraction_results:\n",
        "        with gr.Row():\n",
        "          number_of_articles = gr.Label(label='# of extracted articles')\n",
        "          number_of_sources = gr.Label(label='# of different sources')\n",
        "          average_sentiment = gr.Label(label='Avg sentiment (-1 to 1)')\n",
        "\n",
        "        sources_plot = gr.BarPlot(\n",
        "            x='source',\n",
        "            y='count',\n",
        "            title='Most frequent sources',\n",
        "            x_title='Source',\n",
        "            y_title='Number of articles',\n",
        "            x_label_angle=20\n",
        "        )\n",
        "\n",
        "        categories_plot = gr.BarPlot(\n",
        "            x='Category',\n",
        "            y='proportion',\n",
        "            title='Most frequent categories',\n",
        "            x_title='Category',\n",
        "            y_title='% of ocurrences',\n",
        "            x_label_angle=20\n",
        "        )\n",
        "\n",
        "        extracted_articles = gr.DataFrame(\n",
        "            label='Extracted articles',\n",
        "            headers=['Title', 'Date', 'Source'],\n",
        "            show_search='filter',\n",
        "            col_count=3,\n",
        "            column_widths=['60%', '20%', '20%'],\n",
        "            show_row_numbers=False,\n",
        "            wrap=True,\n",
        "            datatype='html',\n",
        "            interactive=False\n",
        "        )\n",
        "        with gr.Row() as downloads:\n",
        "          download_docs_button = gr.DownloadButton(\n",
        "              label='Download documents',\n",
        "              inputs=search_query,\n",
        "              value=download_docs\n",
        "          )\n",
        "          download_metadata_button = gr.DownloadButton(\n",
        "              label='Download metadata',\n",
        "              inputs=search_query,\n",
        "              value=download_metadata\n",
        "          )\n",
        "\n",
        "  with gr.Tab('Sentiment analysis') as sentiment_analysis_tab:\n",
        "    gr.Markdown('Dive deeper into the sentiment behind brand news. Explore the best and worst articles based on sentiment, view a distribution of overall sentiment, and track how sentiment evolves over time.')\n",
        "    with gr.Row():\n",
        "      gr.Markdown('')\n",
        "      sentiment_results_button = gr.Button('Run sentiment analysis 🔒', interactive=False, size='md')\n",
        "      gr.Markdown('')\n",
        "    with gr.Column():\n",
        "      with gr.Row():\n",
        "        sentiment_by_source = gr.DataFrame(\n",
        "            headers=['Source', 'Average sentiment', 'Frequency']\n",
        "        )\n",
        "        sentiment_distribution_plot = gr.BarPlot(\n",
        "            x='Sentiment',\n",
        "            y='% of ocurrences',\n",
        "            title='Sentiment distribution',\n",
        "            scale=1,\n",
        "            height=400\n",
        "        )\n",
        "      with gr.Row():\n",
        "        with gr.Column():\n",
        "          worst_article_df = gr.DataFrame(\n",
        "              headers=['Variable', 'Worst article'],\n",
        "              column_widths=['20%', '80%']\n",
        "          )\n",
        "          worst_article_summary = gr.Textbox(label='Summary')\n",
        "        with gr.Column():\n",
        "          best_article_df = gr.DataFrame(\n",
        "              headers=['Variable', 'Best article'],\n",
        "              column_widths=['20%', '80%']\n",
        "          )\n",
        "          best_article_summary = gr.Textbox(label='Summary')\n",
        "\n",
        "      sentiment_evolution_plot = gr.Plot(label='Sentiment evolution over time')\n",
        "\n",
        "  with gr.Tab('Topic analysis') as topic_analysis_tab:\n",
        "    gr.Markdown('Uncover the key themes driving conversations around your brand. Articles are automatically clustered into distinct topics with AI-generated names, helping you identify the main areas of focus and trends.')\n",
        "    with gr.Row():\n",
        "      gr.Markdown('')\n",
        "      topic_results_button = gr.Button('Run topic analysis 🔒', interactive=False, size='md')\n",
        "      gr.Markdown('')\n",
        "    gr.Markdown('### Top most relevant themes')\n",
        "    with gr.Column():\n",
        "      with gr.Row():\n",
        "        topic0 = gr.Textbox(label='Topic 0', visible=False)\n",
        "        topic1 = gr.Textbox(label='Topic 1', visible=False)\n",
        "        topic2 = gr.Textbox(label='Topic 2', visible=False)\n",
        "        topic3 = gr.Textbox(label='Topic 3', visible=False)\n",
        "      topic_analysis_barchart = gr.Plot(label='Top words per topic') # 1.75\n",
        "    topic_analysis_scatter = gr.Plot(label='Documents in 2-D space')\n",
        "\n",
        "  # Events #\n",
        "\n",
        "  # main page events\n",
        "  search_button.click(\n",
        "      fn=generate_preliminary_search_results,\n",
        "      inputs=[search_query, n_days_ago, search_filter, app_state],\n",
        "      outputs=[\n",
        "          preliminary_results,\n",
        "          number_of_preliminary_articles,\n",
        "          number_of_preliminary_sources,\n",
        "          newsapi_accordion,\n",
        "          app_state\n",
        "      ]\n",
        "  ).then(\n",
        "        lambda _: gr.update(value='Extract content from articles 🔓', visible=True, interactive=True),\n",
        "        extract_button,\n",
        "        extract_button\n",
        "  )\n",
        "  extract_button.click(\n",
        "      fn=generate_final_results,\n",
        "      inputs=[max_articles, app_state],\n",
        "      outputs=[\n",
        "          extracted_articles,\n",
        "          number_of_articles,\n",
        "          number_of_sources,\n",
        "          average_sentiment,\n",
        "          sources_plot,\n",
        "          categories_plot,\n",
        "          extraction_results,\n",
        "          app_state\n",
        "      ],\n",
        "      show_progress='full',\n",
        "      trigger_mode='once'\n",
        "  ).then(\n",
        "        lambda _: gr.update(value='Extract content from articles 🔒', interactive=False),\n",
        "        extract_button,\n",
        "        extract_button\n",
        "  ).then(\n",
        "        lambda _: gr.update(value='Run sentiment analysis 🔓', interactive=True),\n",
        "        sentiment_results_button,\n",
        "        sentiment_results_button\n",
        "  ).then(\n",
        "      allow_topic_analysis,\n",
        "      number_of_articles,\n",
        "      topic_results_button\n",
        "  )\n",
        "\n",
        "  # sentiment analysis\n",
        "  sentiment_results_button.click(\n",
        "        fn=generate_sentiment_results,\n",
        "        inputs=[app_state],\n",
        "        outputs=[\n",
        "            sentiment_by_source,\n",
        "            sentiment_distribution_plot,\n",
        "            worst_article_df,\n",
        "            worst_article_summary,\n",
        "            best_article_df,\n",
        "            best_article_summary\n",
        "        ]\n",
        "    ).then(\n",
        "        plot_sentiment_over_time,\n",
        "        inputs=[app_state],\n",
        "        outputs=[sentiment_evolution_plot]\n",
        "    ).then(\n",
        "        lambda _: gr.update(value='Run sentiment analysis 🔒', interactive=False),\n",
        "        sentiment_results_button,\n",
        "        sentiment_results_button\n",
        "    )\n",
        "\n",
        "  # topic analysis\n",
        "  topic_results_button.click(\n",
        "      fn=run_topic_analysis,\n",
        "      inputs=[app_state],\n",
        "      outputs=[app_state]\n",
        "  ).then(\n",
        "      fn=generate_topic_results,\n",
        "      inputs=[app_state],\n",
        "      outputs=[\n",
        "          topic_analysis_barchart,\n",
        "          topic_analysis_scatter,\n",
        "          topic0,\n",
        "          topic1,\n",
        "          topic2,\n",
        "          topic3\n",
        "      ]\n",
        "  ).then(\n",
        "        lambda _: gr.update(value='Run topic analysis 🔓', interactive=False),\n",
        "        topic_results_button,\n",
        "        topic_results_button\n",
        "    )\n",
        "\n",
        "\n",
        "demo.queue().launch(share=True, debug=True, allowed_paths=['./downloads'])"
      ],
      "metadata": {
        "id": "PNMi8EzU2grJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "75910de5-86aa-4ad9-e867-eea4c32e0240"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://91d2336be8094138d8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://91d2336be8094138d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://91d2336be8094138d8.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0g8gIIbFqdCR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqROlEO_kzff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}